{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this is a bunch, create a dataframe and then convert it into a numpy array\n",
    "iris_df=pd.DataFrame(iris.data)\n",
    "iris_df['class']=iris.target\n",
    "iris_df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "iris_df.dropna(how=\"all\", inplace=True) # remove any empty lines\n",
    "\n",
    "#shuffle the data\n",
    "iris_df = iris_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#convert the data into the relevant numpy arrays\n",
    "irises=iris_df[['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid']].to_numpy()\n",
    "irises_type=iris_df[\"class\"].to_numpy()\n",
    "\n",
    "#initialise the weights and biases\n",
    "k=10\n",
    "\n",
    "Z1_weights=abs(np.random.randn(4, k)*0.03)\n",
    "Z1_biases=np.zeros((1, k))\n",
    "Z2_weights=np.random.randn(k, 3)*0.03\n",
    "Z2_biases=np.zeros((1, 3))\n",
    "\n",
    "#print(Z1_weights)\n",
    "#print(Z1_biases)\n",
    "#print(Z2_weights)\n",
    "#print(Z2_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(irises, irises_type, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions for further use\n",
    "def softmax(array):\n",
    "    #defines a softmax function on an array of shape (1, 3)\n",
    "    sum=0\n",
    "    for i in array[0]:\n",
    "        sum+=math.e**i\n",
    "    for j in range(len(array[0])):\n",
    "        array[0, j]=math.e**array[0, j]/sum\n",
    "    return array\n",
    "\n",
    "def relu(array):\n",
    "    #defines a relu function on an array of shape (1, k)\n",
    "    return np.maximum(array, 0)\n",
    "\n",
    "def loss(output, correct_output):\n",
    "    #calcusates the Cross-Entropy loss function\n",
    "    #shape of the output is 1x3, and of the correct output is 1x3\n",
    "    if isinstance(output, np.ndarray) and output.shape == (1, 3):\n",
    "        if isinstance(correct_output, np.ndarray) and correct_output.shape == (1, 3):\n",
    "            #print(output)\n",
    "            #print(correct_output)\n",
    "            return -np.dot(np.log(output).reshape(-1), correct_output.reshape(-1))\n",
    "        else:\n",
    "            raise TypeError('wrong array shapes')\n",
    "    else:\n",
    "        raise TypeError('wrong array shapes')\n",
    "\n",
    "\n",
    "def single_forward(example_id, X_train, y_train, Z1_weights, Z1_biases, Z2_weights, Z2_biases):\n",
    "    #Calculates the forward step for a single example using weights&biases as inputs\n",
    "\n",
    "    #example is a numpy array of shape (4, )\n",
    "    example=X_train[example_id]\n",
    "\n",
    "    #define a vector Z1 as weights (k, 4) * example (4,) + biases (10, )\n",
    "    Z1 = np.dot(example, Z1_weights) + Z1_biases\n",
    "   # print('Z1: ', Z1)\n",
    "\n",
    "    # Obtain array of square of each element in x\n",
    "    A1 = relu(Z1)\n",
    "  #  print('A1:', A1)\n",
    "\n",
    "    #define the vector Z2 as weights (3, k) * A1 (k,) + biases (k, )\n",
    "    Z2 = np.dot(A1, Z2_weights) + Z2_biases\n",
    "   # print('Z2:', Z2)\n",
    "    #apply the activation function softmax\n",
    "    output=softmax(Z2) #A2\n",
    "\n",
    "    return Z1, A1, Z2, output\n",
    "     \n",
    "\n",
    "def forward_prop(X_train, y_train, Z1_weights, Z1_biases, Z2_weights, Z2_biases):\n",
    "    #This function is meant to calculate the loss function for an example set\n",
    "    total_loss=0\n",
    "    total_cases=0\n",
    "    #for all vectors in TRAINING SET\n",
    "    for index in range(len(X_train)):\n",
    "        Z1, A1, Z2, output = single_forward(index, X_train, y_train, Z1_weights, Z1_biases, Z2_weights, Z2_biases)\n",
    "\n",
    "        correct_class=y_train[index]\n",
    "        #we need to store the correct class as (1,0,0) or similar np array\n",
    "        correct_output=np.zeros((1, 3))\n",
    "        correct_output[0, correct_class]=1\n",
    "\n",
    "        total_loss+=loss(output, correct_output)\n",
    "        total_cases+=1\n",
    "    return total_loss/total_cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.0760517618808825\n",
      "loss 1.072369158121537\n",
      "loss 1.0683837911515144\n",
      "loss 1.0640564279622777\n",
      "loss 1.0593475906889547\n",
      "loss 1.054216512406087\n",
      "loss 1.048620108287515\n",
      "loss 1.0425120325883928\n",
      "loss 1.0358440552079207\n",
      "loss 1.028569054485753\n",
      "loss 1.020616930552744\n",
      "loss 1.0119171920999095\n",
      "loss 1.0024440465635742\n",
      "loss 0.9923445030426519\n",
      "loss 0.9817406457476221\n",
      "loss 0.9707686727397459\n",
      "loss 0.9593866572950807\n",
      "loss 0.9475986558470193\n",
      "loss 0.9354199997878936\n",
      "loss 0.9229480428826459\n",
      "loss 0.9105603376754919\n",
      "loss 0.8982170088814531\n",
      "loss 0.8858772427619414\n",
      "loss 0.8735030348294898\n",
      "loss 0.8610807565985655\n",
      "loss 0.8485348585436342\n",
      "loss 0.8359063049670921\n",
      "loss 0.8231286311067487\n",
      "loss 0.8103882352470964\n",
      "loss 0.7975315071947425\n",
      "loss 0.7846548170550204\n",
      "loss 0.771426929738868\n",
      "loss 0.7578858891806506\n",
      "loss 0.7442299206747485\n",
      "loss 0.7301734962579364\n",
      "loss 0.7160902782282926\n",
      "loss 0.7018142337743332\n",
      "loss 0.6875619211593029\n",
      "loss 0.673260627833913\n",
      "loss 0.6592172842013645\n",
      "loss 0.6453857778618843\n",
      "loss 0.6318773464145885\n",
      "loss 0.6187854226318307\n",
      "loss 0.6060677304357275\n",
      "loss 0.5939357975248737\n",
      "loss 0.5822838238220586\n",
      "loss 0.5711622613743559\n",
      "loss 0.5605470723475899\n",
      "loss 0.5504326970772154\n",
      "loss 0.5407978149466545\n",
      "loss 0.5316315719331582\n",
      "loss 0.5229034111434192\n",
      "loss 0.5145926557845311\n",
      "loss 0.5066673071297835\n",
      "loss 0.4991123255133872\n",
      "loss 0.49189978790184086\n",
      "loss 0.485004548047882\n",
      "loss 0.4784060195204119\n",
      "loss 0.47208340636175894\n",
      "loss 0.4660153957672826\n",
      "loss 0.46018284285301797\n",
      "loss 0.4545690398681653\n",
      "loss 0.4491575522601154\n",
      "loss 0.4439318728260275\n",
      "loss 0.43887930959720334\n",
      "loss 0.4339857124848873\n",
      "loss 0.42923893703633287\n",
      "loss 0.42462910132165643\n",
      "loss 0.4201450049052955\n",
      "loss 0.41577757561547607\n",
      "loss 0.4115184423392167\n",
      "loss 0.40735906270201067\n",
      "loss 0.4032926314987103\n",
      "loss 0.3993120646049463\n",
      "loss 0.395412131707977\n",
      "loss 0.3915869473399639\n",
      "loss 0.3878316820498947\n",
      "loss 0.38414104228264007\n",
      "loss 0.38051065357825925\n",
      "loss 0.3769367237993175\n",
      "loss 0.3734155015547294\n",
      "loss 0.36994368708877223\n",
      "loss 0.36651877535201544\n",
      "loss 0.3631387718046832\n",
      "loss 0.3598003811435921\n",
      "loss 0.35650144441638\n",
      "loss 0.3532404673347194\n",
      "loss 0.3500152269013969\n",
      "loss 0.34682393548464746\n",
      "loss 0.3436651047043599\n",
      "loss 0.340537447587908\n",
      "loss 0.33744078740072275\n",
      "loss 0.3343744279118617\n",
      "loss 0.331336766943863\n",
      "loss 0.32832693986265415\n",
      "loss 0.3253446487250996\n",
      "loss 0.32238933947293275\n",
      "loss 0.31945997823525707\n",
      "loss 0.3165558668223491\n",
      "loss 0.31367655447298853\n",
      "loss 0.31082171648685936\n",
      "loss 0.307991086524171\n",
      "loss 0.30518469741024956\n",
      "loss 0.30240302794306956\n",
      "loss 0.2996452460645998\n",
      "loss 0.29691103835385524\n",
      "loss 0.29420061700561856\n",
      "loss 0.291514197275645\n",
      "loss 0.2888520066458927\n",
      "loss 0.286213377574182\n",
      "loss 0.2835985207606617\n",
      "loss 0.2810075043949542\n",
      "loss 0.27844060564393425\n",
      "loss 0.275897383931063\n",
      "loss 0.2733780290895879\n",
      "loss 0.270882360739549\n",
      "loss 0.26841039705082564\n",
      "loss 0.2659622635588217\n",
      "loss 0.2635380338426985\n",
      "loss 0.2611375995299237\n",
      "loss 0.2587610002069722\n",
      "loss 0.2564082588151494\n",
      "loss 0.25407986436145064\n",
      "loss 0.2517760105483152\n",
      "loss 0.24949622647367467\n",
      "loss 0.24724049116257757\n",
      "loss 0.24500888451771155\n",
      "loss 0.24280145246750415\n",
      "loss 0.24061863593888025\n",
      "loss 0.2384604851787473\n",
      "loss 0.23632757021544074\n",
      "loss 0.23422076791244467\n",
      "loss 0.23214249805888984\n",
      "loss 0.230097302897158\n",
      "loss 0.22809528646739413\n",
      "loss 0.22615867106631135\n",
      "loss 0.22433253209896925\n",
      "loss 0.22273173634268445\n",
      "loss 0.22157130228219488\n",
      "loss 0.2214862797884629\n",
      "loss 0.22350142090814437\n",
      "loss 0.23154566471917667\n",
      "loss 0.24882947800304311\n",
      "loss 0.2987115690988121\n",
      "loss 0.35834246218786353\n",
      "loss 0.5193934886047339\n",
      "loss 0.4744780054498312\n",
      "loss 0.6310269110570264\n",
      "loss 0.4195157444341902\n",
      "loss 0.5228071239391858\n",
      "loss 0.38555775683553123\n",
      "loss 0.4576918436853812\n",
      "loss 0.3568265123935239\n",
      "loss 0.409315631837314\n",
      "loss 0.33391517389799147\n",
      "loss 0.37406417264730446\n",
      "loss 0.3163545498270888\n",
      "loss 0.3487094762315594\n",
      "loss 0.3028291987058279\n",
      "loss 0.33022326811126845\n",
      "loss 0.2919834137580572\n",
      "loss 0.31622835047526815\n",
      "loss 0.2837433978917006\n",
      "loss 0.30610218180293985\n",
      "loss 0.2773749145017731\n",
      "loss 0.2986882853757993\n",
      "loss 0.2722948402324871\n",
      "loss 0.2931351218581178\n",
      "loss 0.2685760689091531\n",
      "loss 0.28937939370268034\n",
      "loss 0.26574467835801735\n",
      "loss 0.2867720719821831\n",
      "loss 0.2636064400576717\n",
      "loss 0.28501104868184235\n",
      "loss 0.26192199167497315\n",
      "loss 0.28375984713257413\n",
      "loss 0.2603280693399121\n",
      "loss 0.2825623496839438\n",
      "loss 0.2588572402640672\n",
      "loss 0.28143807053314174\n",
      "loss 0.2572007782020602\n",
      "loss 0.2800244561407842\n",
      "loss 0.25553994405414854\n",
      "loss 0.2785366167274946\n",
      "loss 0.2537566465843391\n",
      "loss 0.2768384839653781\n",
      "loss 0.251902140196787\n",
      "loss 0.2749949517082076\n",
      "loss 0.24994289166996256\n",
      "loss 0.272972482967605\n",
      "loss 0.24787548267926116\n",
      "loss 0.2707755153819492\n",
      "loss 0.245703345952268\n",
      "loss 0.2684180233997273\n",
      "loss 0.2434403290868729\n",
      "loss 0.2659262466009848\n",
      "loss 0.24110577127036997\n",
      "loss 0.2633318076210235\n",
      "loss 0.23872064483567934\n",
      "loss 0.26066658431550854\n",
      "loss 0.2363048530121044\n",
      "loss 0.257959338531746\n",
      "loss 0.2338755762854738\n",
      "loss 0.2552338490989729\n",
      "loss 0.23144646061286794\n",
      "loss 0.2525081918390248\n",
      "loss 0.2290274175616304\n",
      "loss 0.24979480869537435\n",
      "loss 0.22662482813952803\n",
      "loss 0.24710106221186412\n",
      "loss 0.22424198307493515\n",
      "loss 0.24443004391699769\n",
      "loss 0.22187963625763615\n",
      "loss 0.24178147441123077\n",
      "loss 0.2195365867017161\n",
      "loss 0.23915258982475646\n",
      "loss 0.2172102348836744\n",
      "loss 0.23653895185224472\n",
      "loss 0.21489708172883507\n",
      "loss 0.23393514839021043\n",
      "loss 0.21259315420227837\n",
      "loss 0.2313353717077205\n",
      "loss 0.21029386337874217\n",
      "loss 0.22871668321734157\n",
      "loss 0.2079832253155504\n",
      "loss 0.22609010594845844\n",
      "loss 0.20566995579122785\n",
      "loss 0.22345203461415947\n",
      "loss 0.2033513009859226\n",
      "loss 0.220788255911605\n",
      "loss 0.20101712613364148\n",
      "loss 0.21810661167998502\n",
      "loss 0.1986737835452625\n",
      "loss 0.21540607471675283\n",
      "loss 0.19632057607481437\n",
      "loss 0.2126861953578989\n",
      "loss 0.1939571859478033\n",
      "loss 0.2099470444541307\n",
      "loss 0.19158363657433589\n",
      "loss 0.20718915062033874\n",
      "loss 0.18920025181138292\n",
      "loss 0.20441343677755425\n",
      "loss 0.18680761565622025\n",
      "loss 0.2016211597344612\n",
      "loss 0.1844065345588215\n",
      "loss 0.19881385540793176\n",
      "loss 0.18199800383357423\n",
      "loss 0.19599329128918225\n",
      "loss 0.17958317904451723\n",
      "loss 0.19316142693535315\n",
      "loss 0.17716335273680156\n",
      "loss 0.19032038260284054\n",
      "loss 0.17473980921696547\n",
      "loss 0.1874677386636036\n",
      "loss 0.17231053973755975\n",
      "loss 0.18460952351665655\n",
      "loss 0.16988012502244504\n",
      "loss 0.18174843927180603\n",
      "loss 0.1674504321476491\n",
      "loss 0.17888731253544124\n",
      "loss 0.1650234306852402\n",
      "loss 0.17602905863626644\n",
      "loss 0.16260239543514202\n",
      "loss 0.1731782704150289\n",
      "loss 0.16018891079564107\n",
      "loss 0.17033749808440693\n",
      "loss 0.1577852611308716\n",
      "loss 0.1675104887463561\n",
      "loss 0.15539219710605928\n",
      "loss 0.16469730444144146\n",
      "loss 0.15301447250826297\n",
      "loss 0.1619049468108968\n",
      "loss 0.15065439828247223\n",
      "loss 0.15913534273874858\n",
      "loss 0.14831333506755878\n",
      "loss 0.15639169007111714\n",
      "loss 0.14599367360022253\n",
      "loss 0.153677225420009\n",
      "loss 0.14369784880442943\n",
      "loss 0.1509952053408006\n",
      "loss 0.14142832653325835\n",
      "loss 0.1483520007916884\n",
      "loss 0.1391903007950836\n",
      "loss 0.14574500679607422\n",
      "loss 0.13698091873277612\n",
      "loss 0.14317943536617564\n",
      "loss 0.13480463567238052\n",
      "loss 0.14065694868281572\n",
      "loss 0.1326629180780949\n",
      "loss 0.13818330393752223\n",
      "loss 0.13056024856237478\n",
      "loss 0.13575923865557316\n",
      "loss 0.12849711758594523\n",
      "loss 0.13338754191028848\n",
      "loss 0.12647568878440524\n",
      "loss 0.13107074456004872\n",
      "loss 0.12449793485363618\n",
      "loss 0.12881108316666914\n",
      "loss 0.12256560929218119\n",
      "loss 0.12661046953839739\n",
      "loss 0.12068022245711271\n",
      "loss 0.12447048940281047\n",
      "loss 0.11884454136529994\n",
      "loss 0.12239408961590073\n",
      "loss 0.11705771158812926\n",
      "loss 0.12038002301763276\n",
      "loss 0.11531181944948654\n",
      "loss 0.11841844211929053\n",
      "loss 0.11361701844177508\n",
      "loss 0.11652122217882478\n",
      "loss 0.1119735425394828\n",
      "loss 0.11468821537237564\n",
      "loss 0.110382817904205\n",
      "loss 0.11291951961069903\n",
      "loss 0.10884181334994517\n",
      "loss 0.11121302355087195\n",
      "loss 0.10735066464904298\n",
      "loss 0.10956763155395367\n",
      "loss 0.10590858808140087\n",
      "loss 0.10798198614935446\n",
      "loss 0.10451459653354873\n",
      "loss 0.10645450661760014\n",
      "loss 0.10316753983513756\n",
      "loss 0.10498234316313171\n",
      "loss 0.10186519368940236\n",
      "loss 0.10356347914481959\n",
      "loss 0.10060603315368291\n",
      "loss 0.1021967202711375\n",
      "loss 0.09938926805494418\n",
      "loss 0.10088004237836372\n",
      "loss 0.09821337085618045\n",
      "loss 0.09961136582432496\n",
      "loss 0.09707676450947633\n",
      "loss 0.09838858219581606\n",
      "loss 0.0959778428000591\n",
      "loss 0.09720957781326099\n",
      "loss 0.09491498840948111\n",
      "loss 0.0960722540282364\n",
      "loss 0.09388658867403953\n",
      "loss 0.09497454441221587\n",
      "loss 0.09289104909026577\n",
      "loss 0.09391442901143862\n",
      "loss 0.09192680467660955\n",
      "loss 0.09288994589529594\n",
      "loss 0.0909923293406981\n",
      "loss 0.09189920025717513\n",
      "loss 0.09008614342686891\n",
      "loss 0.09094037134088759\n",
      "loss 0.08920681963151426\n",
      "loss 0.09001172436954644\n",
      "loss 0.08835376355010995\n",
      "loss 0.08911247774022364\n",
      "loss 0.08752556627649065\n",
      "loss 0.0882409330593016\n",
      "loss 0.0867201050549856\n",
      "loss 0.08739398769658521\n",
      "loss 0.08593566190098545\n",
      "loss 0.086570794966344\n",
      "loss 0.0851685530328019\n",
      "loss 0.08576667045459009\n",
      "loss 0.08442186786786642\n",
      "loss 0.08498501178856126\n",
      "loss 0.08369371266458325\n",
      "loss 0.08422365521101696\n",
      "loss 0.08298318644862876\n",
      "loss 0.08348155763475654\n",
      "loss 0.08228946547390215\n",
      "loss 0.08275599230493857\n",
      "loss 0.08161028314493146\n",
      "loss 0.08204772992245575\n",
      "loss 0.08094629180650913\n",
      "loss 0.08135595302148955\n",
      "loss 0.08029682347330148\n",
      "loss 0.08067990558186623\n",
      "loss 0.07966125847107405\n",
      "loss 0.0800188899374266\n",
      "loss 0.07903902318800136\n",
      "loss 0.0793722636040176\n",
      "loss 0.07842958773244661\n",
      "loss 0.07873943605916743\n",
      "loss 0.07783246352458142\n",
      "loss 0.07811986550067626\n",
      "loss 0.07724720084561117\n",
      "loss 0.07751305560762124\n",
      "loss 0.07667338636554388\n",
      "loss 0.07691855232440978\n",
      "loss 0.07611064066820887\n",
      "loss 0.0763359458590431\n",
      "loss 0.07555891234009726\n",
      "loss 0.0757643469319526\n",
      "loss 0.07501686723507335\n",
      "loss 0.07520387632727886\n",
      "loss 0.07448490115491238\n",
      "loss 0.07465307937012465\n",
      "loss 0.07396080542416725\n",
      "loss 0.0741117771925434\n",
      "loss 0.07344639013909834\n",
      "loss 0.07358088763094635\n",
      "loss 0.07294143936748276\n",
      "loss 0.07306017807145111\n",
      "loss 0.07244575342673655\n",
      "loss 0.07254944822348997\n",
      "loss 0.07195812628272713\n",
      "loss 0.07204735895528393\n",
      "loss 0.07147949228284765\n",
      "loss 0.0715549507566965\n",
      "loss 0.07100968953976608\n",
      "loss 0.07107204720602178\n",
      "loss 0.07054856454190343\n",
      "loss 0.0705984803443351\n",
      "loss 0.07009597079823057\n",
      "loss 0.07013408916659508\n",
      "loss 0.06965177007181338\n",
      "loss 0.06967854212111475\n",
      "loss 0.06921862626083618\n",
      "loss 0.06923521929649419\n",
      "loss 0.06879327208460204\n",
      "loss 0.06880024163441621\n",
      "loss 0.06837584821643321\n",
      "loss 0.06837353926949644\n",
      "loss 0.0679686587802824\n",
      "loss 0.06795813683622066\n",
      "loss 0.06756859286451229\n",
      "loss 0.06755033348904757\n",
      "loss 0.06717557427905739\n",
      "loss 0.06714937376357162\n",
      "loss 0.06678894021682162\n",
      "loss 0.06675587595966853\n",
      "loss 0.06640922166914628\n",
      "loss 0.06636974958797165\n",
      "loss 0.06603634431178187\n",
      "loss 0.06599062350039304\n",
      "loss 0.06566996890951324\n",
      "loss 0.06561868393594857\n",
      "loss 0.0653102609499002\n",
      "loss 0.06525441199632431\n",
      "loss 0.06495763879326487\n",
      "loss 0.0648965283258806\n",
      "loss 0.06461101259565886\n",
      "loss 0.06454369230489444\n",
      "loss 0.06426929753287855\n",
      "loss 0.06419770542488627\n",
      "loss 0.06393365827078099\n",
      "loss 0.06385817230566836\n",
      "loss 0.0636043240460166\n",
      "loss 0.06352532132809217\n",
      "loss 0.06328133027895834\n",
      "loss 0.06319735666073414\n",
      "loss 0.0629626414319315\n",
      "loss 0.06287559104882955\n",
      "loss 0.06265007232481355\n",
      "loss 0.06256029618416917\n",
      "loss 0.06234352247308559\n",
      "loss 0.062251352665263764\n",
      "loss 0.062042886371288776\n",
      "loss 0.061948636020309415\n",
      "loss 0.06174805409187102\n",
      "loss 0.061652017440122706\n",
      "loss 0.06145891273874563\n",
      "loss 0.061361281028720356\n",
      "loss 0.06117527159434836\n",
      "loss 0.06107638026002417\n",
      "loss 0.060897088590471736\n",
      "loss 0.06079720320875508\n",
      "loss 0.06062521356430966\n",
      "loss 0.06052463043675248\n",
      "loss 0.06035843317917355\n",
      "loss 0.06025736762060242\n",
      "loss 0.060099116814629316\n",
      "loss 0.05999803228018641\n",
      "loss 0.059844384472622324\n",
      "loss 0.05974342774627646\n",
      "loss 0.05959414016364979\n",
      "loss 0.059493446633927964\n",
      "loss 0.05934828657028215\n",
      "loss 0.059247980549787076\n",
      "loss 0.05910672537292237\n",
      "loss 0.0590069250357511\n",
      "loss 0.05887104760318227\n",
      "loss 0.05877206153525244\n",
      "loss 0.058641331576486475\n",
      "loss 0.05854341108584884\n",
      "loss 0.05841522011687287\n",
      "loss 0.058317805905957934\n",
      "loss 0.05819237722655226\n",
      "loss 0.058094270407594774\n",
      "loss 0.05797163960406131\n",
      "loss 0.057874414626538605\n",
      "loss 0.057754440955919396\n",
      "loss 0.05765816128261329\n",
      "loss 0.05754070921029652\n",
      "loss 0.05744543122146299\n",
      "loss 0.05733037057725714\n",
      "loss 0.05723616819305712\n",
      "loss 0.05712370107402809\n",
      "loss 0.057029271215687904\n",
      "loss 0.056919296800623666\n",
      "loss 0.056824774616050115\n",
      "loss 0.05671757344361468\n",
      "loss 0.05662396557498118\n",
      "loss 0.056519103729478506\n",
      "loss 0.05642647431913511\n",
      "loss 0.05632431772610603\n",
      "loss 0.05623272158584742\n",
      "loss 0.056132561254642215\n",
      "loss 0.05604183267250444\n",
      "loss 0.05594361323492149\n",
      "loss 0.05585392453217352\n",
      "loss 0.05575757793032771\n",
      "loss 0.055668755593743104\n",
      "loss 0.055574213327387365\n",
      "loss 0.0554863865950964\n",
      "loss 0.055393561524824865\n",
      "loss 0.055306772815478474\n",
      "loss 0.05521557899664613\n",
      "loss 0.055129830981535406\n",
      "loss 0.05504024500238144\n",
      "loss 0.05495518651278723\n",
      "loss 0.05486716265276297\n",
      "loss 0.05478320150285959\n",
      "loss 0.05469665547724367\n",
      "loss 0.05461357524896354\n",
      "loss 0.05452844721154062\n",
      "loss 0.054446435372312155\n",
      "loss 0.05436264874399817\n",
      "loss 0.05428159694579044\n",
      "loss 0.05419909073238977\n",
      "loss 0.054118986118235304\n",
      "loss 0.05403770336512441\n",
      "loss 0.05395853155148323\n",
      "loss 0.053878419123670473\n",
      "loss 0.05380021031507526\n",
      "loss 0.053721584818289314\n",
      "loss 0.05364436704156299\n",
      "loss 0.05356666456029281\n",
      "loss 0.05349042078633751\n",
      "loss 0.053413604977089184\n",
      "loss 0.05333831775355213\n",
      "loss 0.053262461153510895\n",
      "loss 0.053187774718617464\n",
      "loss 0.05311287677044969\n",
      "loss 0.05303901962460898\n",
      "loss 0.05296504542037462\n",
      "loss 0.05289199984141326\n",
      "loss 0.05281891682120211\n",
      "loss 0.052746665127931615\n",
      "loss 0.05267444292494744\n",
      "loss 0.052602967595156584\n",
      "loss 0.052531577886592976\n",
      "loss 0.05246086964314383\n",
      "loss 0.05239032258027255\n",
      "loss 0.05232037177383557\n",
      "loss 0.052250584315263617\n",
      "loss 0.05218135172024919\n",
      "loss 0.05211232260213864\n",
      "loss 0.05204382412639168\n",
      "loss 0.0519755602958263\n",
      "loss 0.05190777909738362\n",
      "loss 0.0518402264414173\n",
      "loss 0.05177311068141677\n",
      "loss 0.05170667927099234\n",
      "loss 0.05164039105053377\n",
      "loss 0.05157438930162522\n",
      "loss 0.05150891931637234\n",
      "loss 0.05144351248035398\n",
      "loss 0.0513784935446174\n",
      "loss 0.05131373709044353\n",
      "loss 0.051249348615745746\n",
      "loss 0.051185226933457235\n",
      "loss 0.051121456024836795\n",
      "loss 0.051057954434742946\n",
      "loss 0.05099481031473776\n",
      "loss 0.05093204092708599\n",
      "loss 0.05086955806399685\n",
      "loss 0.05080732240958591\n",
      "loss 0.05074539987462033\n",
      "loss 0.050683745379481435\n",
      "loss 0.05062239362191775\n",
      "loss 0.050561308540154525\n",
      "loss 0.05050051701857704\n",
      "loss 0.05043999022797711\n",
      "loss 0.050379748832803044\n",
      "loss 0.050319769785914105\n",
      "loss 0.05026006882116169\n",
      "loss 0.050200627499451124\n",
      "loss 0.05014145766300567\n",
      "loss 0.050082544534092895\n",
      "loss 0.050023896897055505\n",
      "loss 0.04996550287497389\n",
      "loss 0.04990736886058931\n",
      "loss 0.04984948526900262\n",
      "loss 0.04979185669797432\n",
      "loss 0.04973447646955349\n",
      "loss 0.04967734579270054\n",
      "loss 0.049620459032427545\n",
      "loss 0.04956381861864079\n",
      "loss 0.0495074182538788\n",
      "loss 0.049451259682976305\n",
      "loss 0.049395338519257294\n",
      "loss 0.049339655261433586\n"
     ]
    }
   ],
   "source": [
    "#GRADIENT DESCENT\n",
    "epochs=600\n",
    "lr=0.08\n",
    "\n",
    "#Now, we define a for loop for num_of_steps of gradient descent\n",
    "for step in range(epochs): #might change to 'while True' later\n",
    "     #firstly, shuffle the data to avoid learning from the order\n",
    "     indices = np.random.permutation(len(X_train))\n",
    "\n",
    "     # Shuffle both arrays using the same indices\n",
    "     X_train = X_train[indices]\n",
    "     y_train = y_train[indices]\n",
    "\n",
    "     #begin with defining arrays for the derivatives\n",
    "     dl_dW1=np.zeros((4, k))\n",
    "     dl_dB1=np.zeros((1, k))\n",
    "     dl_dW2=np.zeros((10, 3))\n",
    "     dl_dB2=np.zeros((1, 3))\n",
    "\n",
    "\n",
    "\n",
    "     #sum up the gradients for each example and average them in the end\n",
    "     for index in range(len(X_train)):\n",
    "          answer=y_train[index]\n",
    "          example=X_train[index]\n",
    "          Z1, A1, Z2, A2 = single_forward(index, X_train, y_train, Z1_weights, Z1_biases, Z2_weights, Z2_biases)\n",
    "          #Firstly, define the derivative wrt elements of A2\n",
    "          dl_A2=np.zeros((1, 3))\n",
    "          dl_A2[0, answer]=-1/A2[0, answer]\n",
    "\n",
    "          #Now, define dl_dZ2\n",
    "          dl_dZ2=np.zeros((1,3))\n",
    "          for j in range(3): #To calculate Z_j\n",
    "               if j==answer:\n",
    "                    dl_dZ2[0,j]=A2[0,j]-1\n",
    "               else:\n",
    "                    dl_dZ2[0,j]=A2[0,j]\n",
    "          \n",
    "          #further, define dl_dA1\n",
    "          dl_dA1=np.zeros((1, k))\n",
    "          for i in range(k):\n",
    "              for j in range(3):\n",
    "                   dl_dA1[0,i]=dl_dA1[0,i]+dl_dZ2[0,j]*Z2_weights[i, j]\n",
    "          \n",
    "          #finally, define dl_dZ1\n",
    "          dl_dZ1=np.zeros((1, k))\n",
    "          for i in range(k):\n",
    "               if Z1[0,i]>0:\n",
    "                    dl_dZ1[0,i]=dl_dA1[0,i]\n",
    "               else:\n",
    "                    dl_dZ1[0,i]=0\n",
    "          \n",
    "          #Calculate the derivatives wrt W2&B2 usindg delta_j=dl_dZ_2(j)\n",
    "          for j in range(3):\n",
    "               dl_dB2[0,j]=dl_dB2[0,j]+dl_dZ2[0,j]\n",
    "               for t in range(10):\n",
    "                    dl_dW2[t, j]=dl_dW2[t, j]+dl_dZ2[0,j]*A1[0,t]\n",
    "          #Calculate the derivatives wrt W1&B1 delta_j=dl_dZ_1(j)\n",
    "          for j in range(k):\n",
    "               dl_dB1[0,j]=dl_dB1[0,j]+dl_dZ1[0,j]\n",
    "               for t in range(4):\n",
    "                    dl_dW1[t, j]=dl_dW1[t, j]+dl_dZ1[0,j]*example[t]\n",
    "\n",
    "     #average over all training examples\n",
    "     dl_dW1=dl_dW1/len(y_train)\n",
    "     dl_dB1=dl_dB1/len(y_train)\n",
    "     dl_dW2=dl_dW2/len(y_train)\n",
    "     dl_dB2=dl_dB2/len(y_train)\n",
    "\n",
    "     #redefine the learning rate to align with the magnitude of weights&biases\n",
    "\n",
    "     #Update the weights & biases\n",
    "     Z1_weights =  Z1_weights-lr*dl_dW1\n",
    "     Z1_biases =  Z1_biases-lr*dl_dB1\n",
    "     Z2_weights = Z2_weights-lr*dl_dW2\n",
    "     Z2_biases = Z2_biases-lr*dl_dB2\n",
    "\n",
    "     loss_val = forward_prop(X_train, y_train, Z1_weights, Z1_biases, Z2_weights, Z2_biases)\n",
    "\n",
    "     print('loss', loss_val)\n",
    "                    \n",
    "                    \n",
    "\n",
    "#define a for loop to do gradient descent\n",
    "    #calculate first derivatives\n",
    "    #second derivatives\n",
    "    #third derivatoves\n",
    "    #forth derivatives\n",
    "    #modify the first set of weights and biases\n",
    "    #modify the second set of weights and biases\n",
    "    #call the forwad propagation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9910714285714286\n"
     ]
    }
   ],
   "source": [
    "#Now, we want to verify this on the training set\n",
    "total_examples=len(y_train)\n",
    "correct_examples=0\n",
    "for index in range(len(X_train)):\n",
    "        Z1, A1, Z2, output = single_forward(index, X_train, y_train, Z1_weights, Z1_biases, Z2_weights, Z2_biases)\n",
    "        \n",
    "        prediction=np.argmax(output)\n",
    "        correct_class=y_train[index]\n",
    "\n",
    "        if prediction==correct_class:\n",
    "            correct_examples+=1\n",
    "\n",
    "print(correct_examples/total_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "#Repeat on the test set\n",
    "total_examples=len(y_test)\n",
    "correct_examples=0\n",
    "for index in range(len(X_test)):\n",
    "        Z1, A1, Z2, output = single_forward(index, X_test, y_test, Z1_weights, Z1_biases, Z2_weights, Z2_biases)\n",
    "        \n",
    "        prediction=np.argmax(output)\n",
    "        correct_class=y_test[index]\n",
    "\n",
    "        if prediction==correct_class:\n",
    "            correct_examples+=1\n",
    "\n",
    "print(correct_examples/total_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sid G| UTC â€” Today at 17:33\n",
    "# # utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "# def cmp(s, dt, t):\n",
    "#   ex = torch.all(dt == t.grad).item()\n",
    "#   app = torch.allclose(dt, t.grad)\n",
    "#   maxdiff = (dt - t.grad).abs().max().item()\n",
    "#   print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
