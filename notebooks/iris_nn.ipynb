{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming you have a numpy array of integers representing classes\n",
    "\n",
    "\n",
    "def onehot_helper(Y):\n",
    "    classes = Y\n",
    "\n",
    "    # Determine the number of unique classes\n",
    "    n_classes = np.unique(classes).size\n",
    "\n",
    "    # Create a zero-filled matrix where rows correspond to samples and columns to classes\n",
    "    onehot_encoded = np.zeros((classes.size, n_classes), dtype=int)\n",
    "    # logging.log(\\1, self.level)\n",
    "    # Set the appropriate indices to 1\n",
    "    onehot_encoded[np.arange(classes.size), classes] = 1\n",
    "    return onehot_encoded\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    iris = datasets.load_iris()\n",
    "    X = np.array(iris[\"data\"])\n",
    "    Y = np.array(iris[\"target\"])\n",
    "    # logging.log(\\1, self.level)\n",
    "    # logging.log(\\1, self.level)\n",
    "    # n, n_cols = iris_df.shape\n",
    "    # d = n_cols - 1\n",
    "    rng = np.random.default_rng(seed=2147483647)\n",
    "\n",
    "    # # Shuffle values\n",
    "    permutation = rng.permutation(n)\n",
    "    X = X[permutation]\n",
    "    Y = Y[permutation]\n",
    "    n_train = int(0.80 * n)\n",
    "\n",
    "    X_train = X[0:n_train, :]\n",
    "    Y_train = Y[0:n_train]\n",
    "    X_test = X[n_train:n, :]\n",
    "    Y_test = Y[n_train:n]\n",
    "    # logging.log(\\1, self.level)\n",
    "    # logging.log(\\1, self.level)\n",
    "    # # Create a zero-filled matrix where rows correspond to samples and columns to classes\n",
    "    Y_train_onehot_encoded = onehot_helper(Y_train)\n",
    "    Y_test_onehot_encoded = onehot_helper(Y_test)\n",
    "\n",
    "    # logging.log(\\1, self.level)\n",
    "    # logging.log(\\1, self.level)\n",
    "    # logging.log(\\1, self.level)\n",
    "    # logging.log(\\1, self.level)\n",
    "    return X_train, Y_train_onehot_encoded, X_test, Y_test_onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120, 3) (30, 4) (30, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = prepare_data()\n",
    "logging.log(\\1, self.level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.3, 1.7, 0.5],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [6. , 3.4, 4.5, 1.6]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    name = \"linear\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, learning_rate=0.07, debug_level=logging.INFO):\n",
    "        np.random.seed(42)\n",
    "        self.weights = (\n",
    "            np.random.randn(input_dim, output_dim) * (5 / 3) / ((input_dim) ** 0.5)\n",
    "        )\n",
    "        self.bias = np.random.randn(1, output_dim) * 0.1\n",
    "        self.learning_rate = learning_rate\n",
    "        self.level = debug_level\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert inputs.shape[1] == self.weights.shape[0], \"Input dimensions do not match\"\n",
    "        self.X = inputs  # N * D dimensional array.\n",
    "        return np.dot(self.X, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        # gradwrtoutput is N * C dimensional array.\n",
    "        logging.log(self.level, f\"\\tIncoming Gradient Shape={gradwrtoutput.shape}\")\n",
    "        logging.log(self.level, f\"\\tIncoming Gradient Norm: {np.sqrt(np.sum(np.power(gradwrtoutput, 2), 1))}\")\n",
    "        assert (\n",
    "            gradwrtoutput.shape[0] == self.X.shape[0]\n",
    "        ), f\"Mismatch in first dimension. The dimensions of incoming gradient  = {gradwrtoutput.shape} self.X = {self.X.shape}\"\n",
    "\n",
    "        assert (\n",
    "            gradwrtoutput.shape[1] == self.weights.shape[1]\n",
    "        ), \"Mismatch in  gradient dimension, it should be {0} but is {1}\".format(\n",
    "            self.weights.shape[1], gradwrtoutput.shape[1]\n",
    "        )\n",
    "        # size should always be\n",
    "        self.gradwrtinput = np.dot(self.X.T, gradwrtoutput)\n",
    "        assert self.gradwrtinput.shape == self.weights.shape\n",
    "\n",
    "        # Compute gradients for weights and bias here.\n",
    "        backpropagated_gradient = np.dot(gradwrtoutput, self.weights.T)\n",
    "        # The weights are updated *after* the computation of the gradient\n",
    "        logging.log(self.level, f\"\\t weights before update:\\n\\t{self.weights}\")\n",
    "        logging.log(self.level, f\"\\t bias before :\\n\\t{self.bias}\")\n",
    "        self.weights -= self.learning_rate * self.gradwrtinput\n",
    "        self.bias -= self.learning_rate * gradwrtoutput.sum(0)\n",
    "        logging.log(self.level, f\"\\t Weights after update:\\n\\t{self.weights}\")\n",
    "        logging.log(self.level, f\"\\t Bias:\\n\\t{self.bias}\")\n",
    "        logging.log(self.level, f\"\\tBackpropagated gradient shape {backpropagated_gradient.shape}\")\n",
    "        return backpropagated_gradient\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "\n",
    "    name = \"relu\"\n",
    "\n",
    "    def __init__(self, debug_level = logging.INFO):\n",
    "        self.inputs = None\n",
    "        self.level = debug_level\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        self.inputs = inputs\n",
    "        # np.maximum(0, np.array([[1, 2], [-1, 12], [-2, 3]]))\n",
    "        # array([[ 1,  2],\n",
    "        #    [ 0, 12],\n",
    "        #    [ 0,  3]])\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, gradwrtoutput):  # dLoss/da'\n",
    "        logging.log(self.level, f\"\\tIncoming Gradient shape={gradwrtoutput.shape}\") # Shape is like self.inputs.shape.\n",
    "        # logging.log(self.level, self.inputs.shape)\n",
    "        # The below op is a bit subtle in the complexity it \"hides\" the full jacobian.\n",
    "        # In reality the gradient is the da/dz in the expression: `dloss/dZ = dloss/da * da/dz` = gradwrtoutput * da/dz\n",
    "        # where z is input to the RELU and a is output of the relu. If we take a single example both are vectors.\n",
    "        # da/dz is the local gradient and is actually a jacobian but we never actually compute the full\n",
    "        self.local_gradient = np.where(self.inputs > 0, 1, 0)\n",
    "        backpropagated_gradient = (\n",
    "            gradwrtoutput * self.local_gradient\n",
    "        )  # Element wise product\n",
    "        logging.log(self.level, f\"\\tBackpropagated Gradient shape={backpropagated_gradient.shape}\")\n",
    "        return backpropagated_gradient\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "\n",
    "    def __init__(self, *args, debug_level=logging.INFO):\n",
    "        self.modules = list(args)\n",
    "        self.level = debug_level\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for i, module in enumerate(self.modules):\n",
    "            logging.log(self.level, f'Forward Prop through  {module.name} layer {i+1}')\n",
    "            inputs = module.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        for i, module in enumerate(reversed(self.modules)):\n",
    "            logging.log(self.level, f'Backward Prop through  {module.name} layer {len(self.modules) - i}')\n",
    "            gradwrtoutput = module.backward(gradwrtoutput)\n",
    "        return gradwrtoutput\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    \"\"\"Also contains the softmax and so computes the dLoss/dLogits\"\"\"\n",
    "\n",
    "    def __init__(self, logits, Y, model: Sequential):\n",
    "        self.logits = logits  # N * C dimensional array.\n",
    "        self.Y = Y  # N * C dimensional one-hot encoded array of ground truth.\n",
    "        self.loss, self.activations = ce_loss_helper(logits, Y)\n",
    "        self.model = model\n",
    "\n",
    "    def backward(self):\n",
    "        gradwrtoutput = self.activations - self.Y  #\n",
    "        assert gradwrtoutput.shape == self.Y.shape == self.activations.shape\n",
    "        self.model.backward(gradwrtoutput)\n",
    "        return gradwrtoutput  # N * C dimensional array.\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.loss)\n",
    "\n",
    "\n",
    "def ce_loss_helper(logits, Y):\n",
    "    exps = np.exp(logits - np.max(logits))  # Numeric stability.\n",
    "    probs = exps / np.sum(exps, axis=1, keepdims=True) # Careful with the axis\n",
    "    return -np.mean(np.sum(Y * np.log(probs + 1e-9), axis=1), axis=0), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.035\n",
    "# LOGGING_LEVEL = logging.DEBUG\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "nn_model = Sequential(\n",
    "    Linear(4, 8, learning_rate=LR, debug_level=logging.DEBUG), \n",
    "    ReLU(debug_level=logging.DEBUG), \n",
    "    Linear(8, 3, learning_rate=LR, debug_level=logging.DEBUG),\n",
    "    debug_level=logging.DEBUG,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss= 4.250118412320548\n",
      "Validation Loss= 16.578612669357135\n",
      "Loss= 13.124735029699396\n",
      "Validation Loss= 12.433959501767848\n",
      "Loss= 14.160898321596713\n",
      "Validation Loss= 4.102106969801691\n",
      "Loss= 4.059757110273953\n",
      "Validation Loss= 2.4845954474287684\n",
      "Loss= 2.0785242164009476\n",
      "Validation Loss= 1.6756881126368588\n",
      "Loss= 1.499290782475049\n",
      "Validation Loss= 1.2401635614029363\n",
      "Loss= 1.1370848012678654\n",
      "Validation Loss= 1.0909962343526791\n",
      "Loss= 1.101196357857616\n",
      "Validation Loss= 1.1360384906047591\n",
      "Loss= 1.0970630842887876\n",
      "Validation Loss= 1.1138279778438325\n",
      "Loss= 1.0963545923014864\n",
      "Validation Loss= 1.1237926356910057\n",
      "Loss= 1.0961931540603738\n",
      "Validation Loss= 1.1191168327593581\n",
      "Loss= 1.096160110045188\n",
      "Validation Loss= 1.121268211805386\n",
      "Loss= 1.0961529168444135\n",
      "Validation Loss= 1.1202692813309119\n",
      "Loss= 1.0961513905156752\n",
      "Validation Loss= 1.1207311755848963\n",
      "Loss= 1.096151062235832\n",
      "Validation Loss= 1.1205171882322789\n",
      "Loss= 1.0961509920218948\n",
      "Validation Loss= 1.120616236433261\n",
      "Loss= 1.0961509769598432\n",
      "Validation Loss= 1.1205703711320822\n",
      "Loss= 1.0961509737326651\n",
      "Validation Loss= 1.1205916054836882\n",
      "Loss= 1.0961509730407437\n",
      "Validation Loss= 1.1205817737039585\n",
      "Loss= 1.0961509728924426\n",
      "Validation Loss= 1.120586325759036\n",
      "Loss= 1.0961509728606467\n",
      "Validation Loss= 1.1205842181449017\n",
      "Loss= 1.096150972853833\n",
      "Validation Loss= 1.1205851939675375\n",
      "Loss= 1.096150972852371\n",
      "Validation Loss= 1.1205847421610764\n",
      "Loss= 1.0961509728520582\n",
      "Validation Loss= 1.1205849513473436\n",
      "Loss= 1.0961509728519911\n",
      "Validation Loss= 1.1205848544940753\n",
      "Loss= 1.0961509728519765\n",
      "Validation Loss= 1.1205848993371328\n",
      "Loss= 1.0961509728519732\n",
      "Validation Loss= 1.1205848785747958\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848881877578\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848837369565\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848857976772\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848848435636\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848852853184\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848850807856\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851754845\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851316387\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.120584885151939\n",
      "Loss= 1.096150972851972\n",
      "Validation Loss= 1.1205848851425402\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851468923\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851448768\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.12058488514581\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.120584885145378\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851455782\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851454856\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851455282\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455085\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851455178\n",
      "Loss= 1.0961509728519723\n",
      "Validation Loss= 1.1205848851455131\n",
      "Loss= 1.0961509728519723\n",
      "Validation Loss= 1.1205848851455151\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519723\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519725\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.120584885145515\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455145\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n",
      "Loss= 1.0961509728519727\n",
      "Validation Loss= 1.1205848851455147\n"
     ]
    }
   ],
   "source": [
    "# model = Sequential(Linear(2, 3), ReLU(), Linear(3, 2))\n",
    "# Loop over epochs\n",
    "#   Do a forward pass\n",
    "#   Compute the loss\n",
    "#   Do a backward pass\n",
    "for i in range(100):\n",
    "    logits = nn_model.forward(X_train)  # logits are the penultimate layer's output\n",
    "    loss = CrossEntropyLoss(\n",
    "        logits, Y_train, nn_model\n",
    "    )  # Return the loss, what about the derivative?\n",
    "\n",
    "    loss.backward()\n",
    "    print(\"Loss=\", loss)\n",
    "\n",
    "    logits = nn_model.forward(X_test)\n",
    "\n",
    "    # Measure validation loss\n",
    "    validation_loss_val, validation_probs = ce_loss_helper(logits, Y_test)\n",
    "    print(\"Validation Loss=\", validation_loss_val)\n",
    "\n",
    "# Do the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8126753  2.37655405]\n",
      "[[-1.05771093  0.82254491 -1.22084365]\n",
      " [ 0.2088636  -1.95967012 -1.32818605]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(2, 3)\n",
    "\n",
    "norm = np.sqrt(np.sum(np.power(a, 2), 1))\n",
    "print(norm)\n",
    "print(a)\n",
    "np.where(a > 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up basic configuration for logging\n",
    "# Adjust the level as per your requirement: DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.Logger(\"Jupyter\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log(logging.INFO, \"Some message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Instances of the Logger class represent a single logging channel. A\n",
       "\"logging channel\" indicates an area of an application. Exactly how an\n",
       "\"area\" is defined is up to the application developer. Since an\n",
       "application can have any number of areas, logging channels are identified\n",
       "by a unique string. Application areas can be nested (e.g. an area\n",
       "of \"input processing\" might include sub-areas \"read CSV files\", \"read\n",
       "XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n",
       "channel names are organized into a namespace hierarchy where levels are\n",
       "separated by periods, much like the Java or Python package namespace. So\n",
       "in the instance given above, channel names might be \"input\" for the upper\n",
       "level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n",
       "There is no arbitrary limit to the depth of nesting.\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initialize the logger with a name and an optional level.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/3.11.3/lib/python3.11/logging/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     RootLogger"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.Logger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
